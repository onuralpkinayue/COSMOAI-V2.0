# -*- coding: utf-8 -*-
"""data preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hVMMhFBpW9OfFUDAt3bMHhwWyNwPviyx
"""

# Gerekli kütüphanelerin kurulumu ve import edilmesi
!pip install astropy pandas numpy matplotlib

import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
from astropy import units as u
import matplotlib.pyplot as plt
import glob
import sys

# ==========================================
# 1. AYARLAR
# ==========================================

sn_file_path = 'drive/MyDrive/tez/Pantheon+SH0ES.dat'
gal_folder_path = 'drive/MyDrive/tez/galaxy_data/'  # Klasör adını değiştirdik

# --- SUPERNOVA SÜTUNLARI ---
sn_cols = {
    'ra': 'RA',
    'dec': 'DEC',
    'z': 'zCMB',
    'target': 'MU_SH0ES'
}

# --- GALAKSİ SÜTUNLARI (SQL Çıktısına Göre) ---
# SQL Sorgunuzda: s.z as redshift, p.z (magnitüd) vardı.
gal_cols = {
    'ra': 'ra',          # SQL çıktısı küçük harf
    'dec': 'dec',
    'z': 'redshift',     # DİKKAT: Spektroskopik Redshift sütunu
    'mags': ['u', 'g', 'r', 'i', 'z'] # Fotometrik Magnitüdler (Buradaki 'z' magnitüddür)
}

# Eşleştirme Toleransları
# Galaksiler için açıyı biraz daraltabiliriz (daha hassas eşleşme için)
MAX_SEPARATION_ANGLE = 5 * u.arcmin   # 5 arcmin (Komşuluk yarıçapı)
MAX_REDSHIFT_DIFF = 0.05              # Galaksi ve SN aynı z'de olmalı, tolerans düşük

# ==========================================
# 2. VERİ YÜKLEME VE BİRLEŞTİRME
# ==========================================

def load_and_clean_data():
    print("Veriler yükleniyor...")

    # --- SN VERİSİ ---
    try:
        df_sn = pd.read_csv(sn_file_path, sep=r'\s+', comment='#')
    except:
        df_sn = pd.read_csv(sn_file_path, sep=',')

    # SN Sütun Kontrolü
    if sn_cols['ra'] not in df_sn.columns:
        print(f"UYARI: Supernova dosyasında '{sn_cols['ra']}' sütunu bulunamadı.")
        return None, None

    # SN Duplike Temizliği
    if sn_cols['ra'] in df_sn.columns:
        subset_cols = [sn_cols['ra'], sn_cols['dec'], sn_cols['z']]
        df_sn = df_sn.drop_duplicates(subset=subset_cols)

    print(f"Supernova Verisi: {len(df_sn)} satır.")

    # --- GALAKSİ VERİSİ ---
    all_gal_files = glob.glob(gal_folder_path + "*.csv")

    if len(all_gal_files) == 0:
        raise ValueError("HATA: 'galaksi_datalari' klasöründe hiç .csv dosyası bulunamadı!")

    df_list = []
    for filename in all_gal_files:
        temp_df = pd.read_csv(filename)
        df_list.append(temp_df)

    df_gal = pd.concat(df_list, axis=0, ignore_index=True)
    df_gal.columns = df_gal.columns.str.strip() # Boşluk temizliği

    print(f"Galaksi Verileri Birleştirildi. Toplam Satır: {len(df_gal)}")
    return df_sn, df_gal

# ==========================================
# 3. OUTLIER FİLTRELEME
# ==========================================

def filter_outliers(df, mag_cols):
    df_clean = df.copy()
    print("Outlier temizliği yapılıyor...")

    for col in mag_cols:
        if col not in df_clean.columns: continue

        # Sayısala çevir
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        df_clean = df_clean.dropna(subset=[col])
        # Fiziksel olmayan değerleri at (-9999 vs)
        df_clean = df_clean[(df_clean[col] > -50) & (df_clean[col] < 50)]

    # İstatistiksel Temizlik (%1 - %99)
    for col in mag_cols:
        if col not in df_clean.columns: continue
        Q1 = df_clean[col].quantile(0.01)
        Q3 = df_clean[col].quantile(0.99)
        df_clean = df_clean[(df_clean[col] >= Q1) & (df_clean[col] <= Q3)]

    return df_clean

# ==========================================
# 4. CROSS-MATCHING (EŞLEŞTİRME)
# ==========================================

def match_catalogs(df_sn, df_gal):
    print("\nKataloglar eşleştiriliyor...")

    # Astropy SkyCoord objeleri
    c_sn = SkyCoord(ra=df_sn[sn_cols['ra']].values*u.degree,
                    dec=df_sn[sn_cols['dec']].values*u.degree)

    c_gal = SkyCoord(ra=df_gal[gal_cols['ra']].values*u.degree,
                     dec=df_gal[gal_cols['dec']].values*u.degree)

    # search_around_sky: Bir SN etrafındaki TÜM Galaksileri bulur
    idx_sn, idx_gal, d2d, d3d = c_gal.search_around_sky(c_sn, MAX_SEPARATION_ANGLE)

    print(f"Mekansal (RA/DEC) eşleşme sayısı: {len(idx_sn)}")

    if len(idx_sn) == 0:
        return pd.DataFrame()

    matched_sn = df_sn.iloc[idx_sn].reset_index(drop=True)
    matched_gal = df_gal.iloc[idx_gal].reset_index(drop=True)

    # Birleştirme (Galaksi sütunlarına _GAL suffix ekleyelim)
    merged_df = pd.concat([matched_sn, matched_gal.add_suffix('_GAL')], axis=1)
    merged_df['separation_arcmin'] = d2d.to(u.arcmin).value

    # --- REDSHIFT FİLTRESİ ---
    # SN zCMB vs Galaksi redshift (spektroskopik)
    z_sn_col = sn_cols['z']
    z_gal_col = gal_cols['z'] + '_GAL' # 'redshift_GAL' olacak

    # Redshift farkını hesapla
    z_diff = abs(merged_df[z_sn_col] - merged_df[z_gal_col])
    merged_df = merged_df[z_diff <= MAX_REDSHIFT_DIFF]

    print(f"Redshift filtresi (delta_z < {MAX_REDSHIFT_DIFF}) sonrası nihai veri sayısı: {len(merged_df)}")
    return merged_df

# ==========================================
# 5. ÇALIŞTIRMA
# ==========================================

try:
    df_sn, df_gal = load_and_clean_data()

    if df_sn is not None and df_gal is not None:
        # Outlier temizliği (Magnitüdler üzerinden)
        df_gal = filter_outliers(df_gal, gal_cols['mags'])
        print(f"Temizlik sonrası Galaksi Sayısı: {len(df_gal)}")

        # Eşleştirme
        final_dataset = match_catalogs(df_sn, df_gal)

        if not final_dataset.empty:
            print("\nOluşturulan Veri Setinden Örnek:")

            # Önizleme sütunları
            preview_cols = [sn_cols['ra'], sn_cols['z'], gal_cols['ra']+'_GAL', gal_cols['z']+'_GAL']
            print(final_dataset[preview_cols].head())

            # Kaydet
            final_dataset.to_csv('final_galaxy_training_data.csv', index=False)
            print("\nDosya 'final_galaxy_training_data.csv' olarak kaydedildi.")

            # Grafik: SN z vs Galaksi z (Mükemmel bir doğru çıkmalı)
            plt.figure(figsize=(8, 6))
            z_gal_col = gal_cols['z'] + '_GAL'
            plt.scatter(final_dataset[sn_cols['z']], final_dataset[z_gal_col], alpha=0.5, s=10, c='green')
            plt.xlabel('Supernova Redshift')
            plt.ylabel('Galaksi Redshift')
            plt.title('Supernova - Galaksi Redshift Uyumu')

            # x=y referans çizgisi
            max_z = max(final_dataset[sn_cols['z']].max(), final_dataset[z_gal_col].max())
            plt.plot([0, max_z], [0, max_z], 'r--', label='Tam Uyum (x=y)')

            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.show()
        else:
            print("Veri seti boş döndü. (Redshift veya Konum eşleşmedi)")

except Exception as e:
    print(f"\nBir hata oluştu: {e}")

# ==========================================
# GEREKLİ KÜTÜPHANELER VE AYARLAR
# ==========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from astropy.coordinates import SkyCoord
from astropy import units as u
from astropy.cosmology import Planck18
import glob

# --- YENİ KÜÇÜK YARIÇAP ---
# 5 Mpc yerine 1.5 Mpc yapıyoruz. Çok daha dar ve düzenli bir çevre.
NEW_SEARCH_RADIUS = 1.5 * u.Mpc

# Dosya yolları
sn_file_path = 'drive/MyDrive/Pantheon+SH0ES.dat'
gal_folder_path = 'drive/MyDrive/galaxy_data/'  # Klasör adını değiştirdik

# Sütun haritaları
sn_cols_map = {'ra': ['RA', 'ra'], 'dec': ['DEC', 'dec'], 'z': ['zCMB', 'z', 'Z', 'redshift']}
gal_cols_map = {'ra': ['ra', 'RA', 'obj_ra'], 'dec': ['dec', 'DEC', 'obj_dec'], 'z': ['redshift', 'z', 'Z', 'spec_z']}

# ==========================================
# YARDIMCI FONKSİYONLAR (Kısa tutuldu)
# ==========================================
def find_col(df, candidates):
    for name in candidates:
        if name in df.columns: return name
    return None

def get_3d_coords(df, ra_col, dec_col, z_col):
    # Basitçe 3D SkyCoord objesi döndürür
    df = df[df[z_col] > 0.001].copy()
    dist = Planck18.luminosity_distance(df[z_col].values)
    return SkyCoord(ra=df[ra_col].values*u.degree, dec=df[dec_col].values*u.degree, distance=dist)

def spherical_to_cartesian_relative(center_coord, target_coords):
    # Hedeflerin merkeze göre bağıl (dX, dY, dZ) konumlarını hesaplar
    sep_3d = center_coord.separation_3d(target_coords)
    # Basit bir projeksiyon ile bağıl koordinatları yaklaşık olarak alalım
    # (Tam küresel trigonometri yerine küçük açı yaklaşımı)
    d_ra = (target_coords.ra - center_coord.ra).wrap_at(180*u.degree).rad
    d_dec = (target_coords.dec - center_coord.dec).rad
    dist = center_coord.distance.to(u.Mpc).value

    dx = dist * d_dec * np.cos(center_coord.ra.rad) # Basitleştirilmiş
    dy = dist * d_ra * np.cos(center_coord.dec.rad) # Basitleştirilmiş
    dz = (target_coords.distance - center_coord.distance).to(u.Mpc).value
    return dx, dy, dz

# ==========================================
# ANA İŞLEM: YENİDEN EŞLEŞTİRME VE GÖRSELLEŞTİRME
# ==========================================

# 1. VERİLERİ YÜKLE (Ham halleriyle)
print("Veriler yeniden yükleniyor ve dar yarıçapla eşleştiriliyor...")
try: df_sn_raw = pd.read_csv(sn_file_path, sep=r'\s+', comment='#')
except: df_sn_raw = pd.read_csv(sn_file_path, sep=',')
df_gal_list = [pd.read_csv(f, low_memory=False) for f in glob.glob(gal_folder_path + "*.csv")]
df_gal_raw = pd.concat(df_gal_list, ignore_index=True)

# 2. KOORDİNATLARI HAZIRLA
sn_ra_c, sn_dec_c, sn_z_c = find_col(df_sn_raw, sn_cols_map['ra']), find_col(df_sn_raw, sn_cols_map['dec']), find_col(df_sn_raw, sn_cols_map['z'])
gal_ra_c, gal_dec_c, gal_z_c = find_col(df_gal_raw, gal_cols_map['ra']), find_col(df_gal_raw, gal_cols_map['dec']), find_col(df_gal_raw, gal_cols_map['z'])

c_sn_all = get_3d_coords(df_sn_raw, sn_ra_c, sn_dec_c, sn_z_c)
c_gal_all = get_3d_coords(df_gal_raw, gal_ra_c, gal_dec_c, gal_z_c)

# 3. YENİ EŞLEŞTİRME (1.5 Mpc Yarıçap)
idx_sn, idx_gal, _, _ = c_gal_all.search_around_3d(c_sn_all, NEW_SEARCH_RADIUS)
print(f"Yeni (dar) yarıçapta {len(idx_sn)} eşleşme bulundu.")

# Eşleşmeleri bir DataFrame'de topla
matched_data = pd.DataFrame({
    'sn_idx': idx_sn,
    'gal_idx': idx_gal
})

# 4. ÖRNEK SEÇİMİ (Zengin, Orta, Fakir)
# Her SN'nin kaç komşusu var say
sn_counts = matched_data.groupby('sn_idx').size().sort_values(ascending=False)
unique_sn_indices = sn_counts.index

# 3 Temsili Örnek Seç: En başı, ortası ve sonu
examples_indices = [
    unique_sn_indices[0],                       # En Zengin (Rich Cluster)
    unique_sn_indices[len(unique_sn_indices)//2], # Orta Halli (Group)
    unique_sn_indices[-1]                       # En Fakir (Poor/Field)
]
titles = ['Yoğun Küme Örneği', 'Orta Halli Grup Örneği', 'Tenha Ortam Örneği']

# ==========================================
# GÖRSELLEŞTİRME (YAN YANA 3 GRAFİK)
# ==========================================
plt.style.use('default') # Arka planı beyaz yapalım, noktalar koyu olsun
fig = plt.figure(figsize=(18, 6))

for i, sn_idx in enumerate(examples_indices):
    ax = fig.add_subplot(1, 3, i+1, projection='3d')

    # Merkezdeki SN'yi al
    center_sn_coord = c_sn_all[sn_idx]

    # Komşu Galaksilerin indekslerini bul
    neighbor_gal_indices = matched_data[matched_data['sn_idx'] == sn_idx]['gal_idx'].values
    neighbor_coords = c_gal_all[neighbor_gal_indices]
    count = len(neighbor_coords)

    # Bağıl Koordinatları (dX, dY, dZ) hesapla. Merkez SN = (0,0,0)
    dx, dy, dz = spherical_to_cartesian_relative(center_sn_coord, neighbor_coords)

    # --- ÇİZİM (Görsel İyileştirmeler Burada) ---

    # 1. MERKEZ SN: Parlak Kırmızı, Büyük Yıldız
    ax.scatter([0], [0], [0], c='red', marker='*', s=400, edgecolors='black', zorder=10, label='Supernova')

    # 2. GALAKSİLER: Koyu Mavi/Yeşil, Büyük, Net Noktalar (Alpha yüksek)
    # 'teal' rengi koyu ve belirgindir. s=80 ile büyüttük. alpha=0.8 ile netleştirdik.
    ax.scatter(dx, dy, dz, c='teal', marker='o', s=80, alpha=0.8, edgecolors='darkslategrey', label='Galaksiler')

    # Ayarlar
    ax.set_title(f"{titles[i]}\n(Yarıçap: 1.5 Mpc, Komşu: {count})", fontsize=12, fontweight='bold')
    ax.set_xlabel('dX (Mpc)')
    ax.set_ylabel('dY (Mpc)')
    ax.set_zlabel('dZ (Mpc)')

    # Eksen limitlerini sabitle ki ölçek aynı görünsün (-1.5 ile +1.5 arası)
    limit = NEW_SEARCH_RADIUS.value
    ax.set_xlim(-limit, limit); ax.set_ylim(-limit, limit); ax.set_zlim(-limit, limit)

    # İlk grafiğe lejant koy
    if i == 0: ax.legend()

    # Açıyı ayarla
    ax.view_init(elev=25, azim=135)

plt.tight_layout()
plt.show()

# --- Gerekli Kütüphaneler ---
!pip install astropy pandas numpy matplotlib catboost -q

import pandas as pd
import numpy as np
import glob
from astropy.coordinates import SkyCoord
from astropy import units as u
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. SABİT AYARLAR VE VERİ YÜKLEME (Tek Seferlik)
# ==========================================

sn_file_path = 'drive/MyDrive/tez/Pantheon+SH0ES.dat'
gal_folder_path = 'drive/MyDrive/tez/galaxy_data/'

# --- SÜTUN İSİMLERİ ---
sn_cols = {'ra': 'RA', 'dec': 'DEC', 'z': 'zCMB', 'target': 'MU_SH0ES', 'sn_id': 'SNID'}
# Not: Pantheon dosyasında SN ismini tutan bir sütun varsa sn_id'ye ekleyin, yoksa kod index kullanır.

gal_cols = {'ra': 'ra', 'dec': 'dec', 'z': 'redshift', 'mags': ['u', 'g', 'r', 'i', 'z']}

def load_raw_data():
    print("Ham veriler yükleniyor (Bu işlem bir kez yapılır)...")
    # SN Yükleme
    try:
        df_sn = pd.read_csv(sn_file_path, sep=r'\s+', comment='#')
    except:
        df_sn = pd.read_csv(sn_file_path, sep=',')

    # SN ID sütunu kontrolü (Yoksa indexi ID yapalım)
    if 'SNID' not in df_sn.columns and 'CID' in df_sn.columns:
         df_sn['SNID'] = df_sn['CID'] # Pantheon formatı
    elif 'SNID' not in df_sn.columns:
         df_sn['SNID'] = df_sn.index

    # Galaksi Yükleme
    all_gal_files = glob.glob(gal_folder_path + "*.csv")
    df_list = [pd.read_csv(f) for f in all_gal_files]
    df_gal = pd.concat(df_list, axis=0, ignore_index=True)
    df_gal.columns = df_gal.columns.str.strip()

    # Basit temizlik (NaN ve anlamsız değerler)
    for col in gal_cols['mags']:
        df_gal = df_gal[pd.to_numeric(df_gal[col], errors='coerce').notnull()]
        df_gal = df_gal[(df_gal[col] > -50) & (df_gal[col] < 50)]

    print(f"Veriler Hazır. SN: {len(df_sn)}, Galaksi: {len(df_gal)}")
    return df_sn, df_gal

# Verileri belleğe alalım
global_df_sn, global_df_gal = load_raw_data()

# ==========================================
# 2. DENEY FONKSİYONU (Preprocessing + Modelleme)
# ==========================================

def run_experiment(max_angle_arcmin, max_z_diff, outlier_quantile=(0.01, 0.99)):
    """
    Belirli parametrelerle eşleştirme yapar ve model başarısını döndürür.
    """

    # --- A) OUTLIER TEMİZLİĞİ (Parametrik) ---
    df_gal_clean = global_df_gal.copy()
    q_low, q_high = outlier_quantile
    for col in gal_cols['mags']:
        lower = df_gal_clean[col].quantile(q_low)
        upper = df_gal_clean[col].quantile(q_high)
        df_gal_clean = df_gal_clean[(df_gal_clean[col] >= lower) & (df_gal_clean[col] <= upper)]

    # --- B) CROSS MATCHING ---
    # Astropy koordinatları
    c_sn = SkyCoord(ra=global_df_sn[sn_cols['ra']].values*u.degree, dec=global_df_sn[sn_cols['dec']].values*u.degree)
    c_gal = SkyCoord(ra=df_gal_clean[gal_cols['ra']].values*u.degree, dec=df_gal_clean[gal_cols['dec']].values*u.degree)

    # Eşleştirme
    idx_sn, idx_gal, d2d, _ = c_gal.search_around_sky(c_sn, max_angle_arcmin * u.arcmin)

    if len(idx_sn) < 50: # Çok az veri kaldıysa deneyi iptal et
        return None

    matched_sn = global_df_sn.iloc[idx_sn].reset_index(drop=True)
    matched_gal = df_gal_clean.iloc[idx_gal].reset_index(drop=True)

    merged = pd.concat([matched_sn, matched_gal.add_suffix('_GAL')], axis=1)

    # --- C) REDSHIFT FİLTRESİ ---
    z_diff = abs(merged[sn_cols['z']] - merged[gal_cols['z']+'_GAL'])
    merged = merged[z_diff <= max_z_diff]

    if len(merged) < 50: return None

    # --- D) FEATURE ENGINEERING (Renkler) ---
    # Modelin başarısı için kritik adım
    merged['u_g'] = merged['u_GAL'] - merged['g_GAL']
    merged['g_r'] = merged['g_GAL'] - merged['r_GAL']
    merged['r_i'] = merged['r_GAL'] - merged['i_GAL']
    merged['i_z'] = merged['r_GAL'] - merged['z_GAL'] # i-z yerine r-z de denenebilir ama standart kalalım

    feature_cols = ['u_GAL', 'g_GAL', 'r_GAL', 'i_GAL', 'z_GAL', 'u_g', 'g_r', 'r_i', 'i_z']
    target = sn_cols['target']

    # --- E) MODELLEME (CatBoost) ---
    # Data Leakage önlemek için SN ID bazlı split
    sn_ids = merged['SNID'].unique()
    train_ids, test_ids = train_test_split(sn_ids, test_size=0.3, random_state=42)

    train_df = merged[merged['SNID'].isin(train_ids)]
    test_df = merged[merged['SNID'].isin(test_ids)]

    X_train = train_df[feature_cols]
    y_train = train_df[target]
    X_test = test_df[feature_cols]

    # Test setindeki gerçek değerleri (Aggregation öncesi) sakla
    # Not: Hızlı olması için burada aggregation yapmadan direkt RMSE hesaplayacağız.
    # (Daha detaylı analizde aggregation eklenebilir ama karşılaştırma için bu yeterli)
    y_test = test_df[target]

    model = CatBoostRegressor(iterations=500, depth=6, learning_rate=0.05, verbose=0, random_state=42, allow_writing_files=False)
    model.fit(X_train, y_train)

    preds = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)

    return {
        'Angle (arcmin)': max_angle_arcmin,
        'Z Threshold': max_z_diff,
        'Data Count': len(merged),
        'Unique SN': len(merged['SNID'].unique()),
        'RMSE': rmse,
        'R2 Score': r2
    }

# ==========================================
# 3. KONFİGÜRASYON TARAMASI (GRID SEARCH)
# ==========================================

# Denenecek Parametre Listeleri
angle_configs = [1, 3, 5, 10]         # 1 arcmin (çok sıkı) -> 10 arcmin (çok geniş)
z_diff_configs = [0.005, 0.01, 0.05, 0.1] # Redshift ne kadar benzemeli?

results = []
print(f"Toplam {len(angle_configs) * len(z_diff_configs)} farklı konfigürasyon deneniyor...")

for angle in angle_configs:
    for z_diff in z_diff_configs:
        print(f"Deneneniyor -> Açı: {angle}' | Z_Fark: {z_diff} ...", end=" ")

        res = run_experiment(angle, z_diff)

        if res:
            results.append(res)
            print(f"R2: {res['R2 Score']:.4f}")
        else:
            print("Yetersiz veri, atlandı.")

# ==========================================
# 4. SONUÇLARIN ANALİZİ
# ==========================================

results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by='R2 Score', ascending=False)

print("\n" + "="*40)
print("EN İYİ PREPROCESSING KONFİGÜRASYONLARI")
print("="*40)
print(results_df.head(10))

# En iyiyi seçip detaylı bilgi verelim
best_config = results_df.iloc[0]
print(f"\n✅ KAZANAN KONFİGÜRASYON:")
print(f"   Maksimum Açı: {best_config['Angle (arcmin)']} arcmin")
print(f"   Maksimum Redshift Farkı: {best_config['Z Threshold']}")
print(f"   Elde Edilen R2 Skoru: {best_config['R2 Score']:.4f}")
print(f"   Elde Edilen RMSE: {best_config['RMSE']:.4f}")

# --- Isı Haritası (Heatmap) ---
pivot_table = results_df.pivot(index='Angle (arcmin)', columns='Z Threshold', values='R2 Score')
plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt=".3f")
plt.title('Hangi Parametreler Modeli İyileştiriyor? (R2 Skoru)')
plt.show()

# --- Gerekli Kütüphaneler ---
!pip install astropy pandas numpy matplotlib catboost -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob
from astropy.coordinates import SkyCoord
from astropy import units as u
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# ==========================================
# 1. KAZANAN AYARLAR (GOLD STANDARD)
# ==========================================
# Grid Search sonucunda bulduğumuz en iyi değerler:
MAX_SEPARATION_ANGLE = 3.0   # arcmin
MAX_REDSHIFT_DIFF = 0.005    # z farkı

sn_file_path = 'drive/MyDrive/tez/Pantheon+SH0ES.dat'
gal_folder_path = 'drive/MyDrive/tez/galaxy_data/'

# ==========================================
# 2. VERİ HAZIRLIĞI VE EŞLEŞTİRME
# ==========================================

def prepare_final_dataset():
    print(f"Veriler hazırlanıyor... (Açı: {MAX_SEPARATION_ANGLE}', Z_Fark: {MAX_REDSHIFT_DIFF})")

    # --- SN YÜKLEME ---
    try:
        df_sn = pd.read_csv(sn_file_path, sep=r'\s+', comment='#')
    except:
        df_sn = pd.read_csv(sn_file_path, sep=',')

    # SN ID Kontrolü
    if 'SNID' not in df_sn.columns:
        df_sn['SNID'] = df_sn['CID'] if 'CID' in df_sn.columns else df_sn.index

    # Sütun İsimleri (Dosyana göre ayarlı)
    sn_ra, sn_dec, sn_z, sn_target = 'RA', 'DEC', 'zCMB', 'MU_SH0ES'

    # --- GALAKSİ YÜKLEME ---
    all_files = glob.glob(gal_folder_path + "*.csv")
    df_gal = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)
    df_gal.columns = df_gal.columns.str.strip() # Boşluk temizliği

    # Temel Temizlik
    mags = ['u', 'g', 'r', 'i', 'z']
    for m in mags:
        df_gal = df_gal[pd.to_numeric(df_gal[m], errors='coerce').notnull()]
        df_gal = df_gal[(df_gal[m] > -50) & (df_gal[m] < 50)]

    # --- EŞLEŞTİRME (CROSS-MATCH) ---
    c_sn = SkyCoord(ra=df_sn[sn_ra].values*u.deg, dec=df_sn[sn_dec].values*u.deg)
    c_gal = SkyCoord(ra=df_gal['ra'].values*u.deg, dec=df_gal['dec'].values*u.deg)

    idx_sn, idx_gal, d2d, _ = c_gal.search_around_sky(c_sn, MAX_SEPARATION_ANGLE * u.arcmin)

    matched = pd.concat([
        df_sn.iloc[idx_sn].reset_index(drop=True),
        df_gal.iloc[idx_gal].reset_index(drop=True).add_suffix('_GAL')
    ], axis=1)

    # Redshift Filtresi (Kritik Adım)
    matched = matched[abs(matched[sn_z] - matched['redshift_GAL']) <= MAX_REDSHIFT_DIFF]

    print(f"Eşleşen ve Filtrelenen Veri Sayısı: {len(matched)}")
    return matched

# Veriyi Çek
df = prepare_final_dataset()

# ==========================================
# 3. FEATURE ENGINEERING & MODEL
# ==========================================

if df is not None and len(df) > 50:
    # Renkleri Ekle (Fiziksel Bilgi)
    df['u_g'] = df['u_GAL'] - df['g_GAL']
    df['g_r'] = df['g_GAL'] - df['r_GAL']
    df['r_i'] = df['r_GAL'] - df['i_GAL']
    df['i_z'] = df['r_GAL'] - df['z_GAL']

    features = ['u_GAL', 'g_GAL', 'r_GAL', 'i_GAL', 'z_GAL', 'u_g', 'g_r', 'r_i', 'i_z']
    target = 'MU_SH0ES'

    # Süpernova bazlı Split (Data Leakage Önleme)
    sn_ids = df['SNID'].unique()
    train_ids, test_ids = train_test_split(sn_ids, test_size=0.3, random_state=42)

    train_df = df[df['SNID'].isin(train_ids)]
    test_df = df[df['SNID'].isin(test_ids)]

    X_train = train_df[features]
    y_train = train_df[target]

    # Test verisi (Prediction için)
    X_test = test_df[features]

    # --- MODEL EĞİTİMİ (CatBoost) ---
    print("Model eğitiliyor...")
    model = CatBoostRegressor(iterations=1000, depth=6, learning_rate=0.05, verbose=0, random_state=42)
    model.fit(X_train, y_train)

    # --- TAHMİN VE AGGREGATION ---
    # Tahminleri yap
    preds = model.predict(X_test)

    # Tahminleri test setine ekle ve ortalama al
    results = test_df.copy()
    results['Predicted_MU'] = preds

    final_results = results.groupby('SNID').agg({
        'zCMB': 'mean',          # X ekseni için
        'MU_SH0ES': 'mean',      # Gerçek Y (Target)
        'Predicted_MU': 'mean'   # Tahmin Edilen Y
    }).reset_index()

    # --- METRİKLER ---
    rmse = np.sqrt(mean_squared_error(final_results['MU_SH0ES'], final_results['Predicted_MU']))
    r2 = r2_score(final_results['MU_SH0ES'], final_results['Predicted_MU'])

    print(f"\n--- NİHAİ SONUÇLAR ---")
    print(f"RMSE: {rmse:.4f}")
    print(f"R2 Skoru: {r2:.4f}")

    # ==========================================
    # 4. FİNAL GRAFİĞİ (GÖRSELLEŞTİRME)
    # ==========================================
    plt.figure(figsize=(10, 6))

    # Gerçek Veriler (Mavi Daireler)
    plt.scatter(final_results['zCMB'], final_results['MU_SH0ES'],
                c='royalblue', s=80, alpha=0.7, edgecolors='k', label='Gerçek (Real)')

    # Tahmin Verileri (Kırmızı Çarpılar)
    plt.scatter(final_results['zCMB'], final_results['Predicted_MU'],
                c='red', marker='x', s=80, linewidth=2, label='Tahmin (Prediction)')

    # Hata Çizgileri
    plt.vlines(final_results['zCMB'], final_results['MU_SH0ES'], final_results['Predicted_MU'],
               colors='gray', alpha=0.3)

    plt.title(f"Final Model: Redshift vs MU_SH0ES\n(Açı: 3', dZ: 0.005) -> R2: {r2:.3f}", fontsize=14)
    plt.xlabel("Redshift (zCMB)", fontsize=12)
    plt.ylabel("Distance Modulus (MU_SH0ES)", fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

    # Feature Importance
    feat_imp = model.get_feature_importance()
    idx = np.argsort(feat_imp)[::-1]
    plt.figure(figsize=(10,4))
    plt.bar(range(len(features)), feat_imp[idx], align='center', color='orange', edgecolor='k')
    plt.xticks(range(len(features)), np.array(features)[idx], rotation=45)
    plt.title("Özellik Önemi (Feature Importance)")
    plt.show()

else:
    print("Yetersiz veri! Lütfen klasör yollarını veya eşleşme kriterlerini kontrol edin.")

# --- 1. GEREKLİ KÜTÜPHANELERİ KURMA ---
!pip install astropy pandas numpy matplotlib seaborn catboost xgboost lightgbm -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob
from astropy.coordinates import SkyCoord
from astropy import units as u

# Modeller
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

# Preprocessing & Metrics
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# ==========================================
# 2. AYARLAR (KAZANAN KONFİGÜRASYON)
# ==========================================
# Grid Search'ten gelen en iyi değerler:
MAX_SEPARATION_ANGLE = 3.0   # arcmin
MAX_REDSHIFT_DIFF = 0.005    # z farkı

sn_file_path = 'drive/MyDrive/tez/Pantheon+SH0ES.dat'
gal_folder_path = 'drive/MyDrive/tez/galaxy_data/'

# ==========================================
# 3. VERİ HAZIRLIĞI (AYNI FİLTRELERLE)
# ==========================================
def load_data():
    print(f"Veri hazırlanıyor (Açı: {MAX_SEPARATION_ANGLE}', dZ: {MAX_REDSHIFT_DIFF})...")

    # SN Yükle
    try:
        df_sn = pd.read_csv(sn_file_path, sep=r'\s+', comment='#')
    except:
        df_sn = pd.read_csv(sn_file_path, sep=',')

    # SNID sütunu yoksa oluştur
    if 'SNID' not in df_sn.columns:
        df_sn['SNID'] = df_sn['CID'] if 'CID' in df_sn.columns else df_sn.index

    # Galaksi Yükle
    all_files = glob.glob(gal_folder_path + "*.csv")
    df_gal = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)
    df_gal.columns = df_gal.columns.str.strip()

    # Temizlik (Outlier ve NaN temizliği)
    for m in ['u', 'g', 'r', 'i', 'z']:
        df_gal = df_gal[pd.to_numeric(df_gal[m], errors='coerce').notnull()]
        df_gal = df_gal[(df_gal[m] > -50) & (df_gal[m] < 50)]

    # Eşleştirme (Cross-Match)
    c_sn = SkyCoord(ra=df_sn['RA'].values*u.deg, dec=df_sn['DEC'].values*u.deg)
    c_gal = SkyCoord(ra=df_gal['ra'].values*u.deg, dec=df_gal['dec'].values*u.deg)

    idx_sn, idx_gal, _, _ = c_gal.search_around_sky(c_sn, MAX_SEPARATION_ANGLE * u.arcmin)

    matched = pd.concat([
        df_sn.iloc[idx_sn].reset_index(drop=True),
        df_gal.iloc[idx_gal].reset_index(drop=True).add_suffix('_GAL')
    ], axis=1)

    # Z Filtresi
    matched = matched[abs(matched['zCMB'] - matched['redshift_GAL']) <= MAX_REDSHIFT_DIFF]

    # Feature Engineering (Renkler)
    matched['u_g'] = matched['u_GAL'] - matched['g_GAL']
    matched['g_r'] = matched['g_GAL'] - matched['r_GAL']
    matched['r_i'] = matched['r_GAL'] - matched['i_GAL']
    matched['i_z'] = matched['r_GAL'] - matched['z_GAL']

    return matched

# Veriyi yükle
df = load_data()

# ==========================================
# 4. MODELLERİN TANIMLANMASI
# ==========================================
# Farklı algoritmaları sözlük yapısında tanımlıyoruz
models_to_test = {
    "Linear Regression": Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),

    "XGBoost": XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42),

    "CatBoost": CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=6, verbose=0, random_state=42)
}

# ==========================================
# 5. BENCHMARK DÖNGÜSÜ
# ==========================================
if df is not None and len(df) > 50:
    results = []
    features = ['u_GAL', 'g_GAL', 'r_GAL', 'i_GAL', 'z_GAL', 'u_g', 'g_r', 'r_i', 'i_z']
    target = 'MU_SH0ES'

    # Split (Data Leakage Önleme - SNID bazlı)
    sn_ids = df['SNID'].unique()
    train_ids, test_ids = train_test_split(sn_ids, test_size=0.3, random_state=42)

    train_df = df[df['SNID'].isin(train_ids)]
    test_df = df[df['SNID'].isin(test_ids)]

    X_train = train_df[features]
    y_train = train_df[target]
    X_test = test_df[features]

    print(f"\n--- MODEL KARŞILAŞTIRMA BAŞLIYOR ({len(models_to_test)} Model) ---")

    for name, model in models_to_test.items():
        print(f"Eğitiliyor: {name}...")

        # Eğit
        model.fit(X_train, y_train)

        # Tahmin (Galaksi bazlı)
        preds = model.predict(X_test)

        # Aggregation (SNID bazlı ortalama alma)
        temp_res = test_df.copy()
        temp_res['preds'] = preds
        agg_res = temp_res.groupby('SNID').agg({target: 'mean', 'preds': 'mean'}).reset_index()

        # Metrikler
        rmse = np.sqrt(mean_squared_error(agg_res[target], agg_res['preds']))
        r2 = r2_score(agg_res[target], agg_res['preds'])

        results.append({'Model': name, 'RMSE': rmse, 'R2 Score': r2})

    # ==========================================
    # 6. SONUÇLARIN GÖRSELLEŞTİRİLMESİ
    # ==========================================
    results_df = pd.DataFrame(results).sort_values(by='R2 Score', ascending=False)

    print("\n" + "="*40)
    print("BENCHMARK SONUÇLARI")
    print("="*40)
    print(results_df)

    # Grafik Çizimi
    plt.figure(figsize=(10, 6))
    sns.barplot(x='R2 Score', y='Model', data=results_df, palette='viridis')
    plt.title('Makine Öğrenmesi Modelleri Karşılaştırması (R2 Skoru)')
    plt.xlabel('R2 Skoru (Daha yüksek daha iyi)')
    plt.xlim(0, 1.0) # 0 ile 1 arası sabitle
    plt.grid(axis='x', linestyle='--', alpha=0.5) # Düzeltilen satır burası

    # Barların ucuna değerleri yazma
    for index, row in results_df.reset_index().iterrows():
        plt.text(row['R2 Score'], index, f"{row['R2 Score']:.3f}", va='center', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.show()

else:
    print("Yetersiz veri veya dosya yolu hatası.")

!pip install catboost astropy pandas numpy matplotlib scikit-learn -q

# --- 1. GEREKLİ KÜTÜPHANELER ---
import pandas as pd
import numpy as np
import glob
from astropy.coordinates import SkyCoord
from astropy import units as u

# Google Colab dosya indirme modülü
try:
    from google.colab import files
except ImportError:
    pass

# ==========================================
# 2. AYARLAR (KAZANAN KONFİGÜRASYON)
# ==========================================
# Grid Search ile bulduğun en iyi değerler:
MAX_SEPARATION_ANGLE = 3.0   # arcmin
MAX_REDSHIFT_DIFF = 0.005    # z farkı

sn_file_path = 'drive/MyDrive/tez/Pantheon+SH0ES.dat'
gal_folder_path = 'drive/MyDrive/tez/galaxy_data/'

# ==========================================
# 3. VERİ SETİ OLUŞTURMA FONKSİYONU
# ==========================================
def create_and_save_dataset():
    print(f"Dataset oluşturuluyor... (Kriterler -> Açı: {MAX_SEPARATION_ANGLE}', dZ: {MAX_REDSHIFT_DIFF})")

    # --- A) Supernova Verisini Yükle ---
    try:
        df_sn = pd.read_csv(sn_file_path, sep=r'\s+', comment='#')
    except:
        df_sn = pd.read_csv(sn_file_path, sep=',')

    # SNID Sütunu (Gruplama için kritik)
    if 'SNID' not in df_sn.columns:
        df_sn['SNID'] = df_sn['CID'] if 'CID' in df_sn.columns else df_sn.index

    print(f"Supernova verisi yüklendi: {len(df_sn)} satır.")

    # --- B) Galaksi Verilerini Yükle ---
    all_files = glob.glob(gal_folder_path + "*.csv")
    df_gal = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)
    df_gal.columns = df_gal.columns.str.strip()

    # Temel Temizlik (Anlamsız değerleri at)
    for m in ['u', 'g', 'r', 'i', 'z']:
        df_gal = df_gal[pd.to_numeric(df_gal[m], errors='coerce').notnull()]
        df_gal = df_gal[(df_gal[m] > -50) & (df_gal[m] < 50)]

    print(f"Galaksi verisi yüklendi ve temizlendi: {len(df_gal)} satır.")

    # --- C) Eşleştirme (Cross-Match) ---
    c_sn = SkyCoord(ra=df_sn['RA'].values*u.deg, dec=df_sn['DEC'].values*u.deg)
    c_gal = SkyCoord(ra=df_gal['ra'].values*u.deg, dec=df_gal['dec'].values*u.deg)

    # search_around_sky ile belirli yarıçap içindekileri bul
    idx_sn, idx_gal, d2d, _ = c_gal.search_around_sky(c_sn, MAX_SEPARATION_ANGLE * u.arcmin)

    # Verileri Birleştir
    matched = pd.concat([
        df_sn.iloc[idx_sn].reset_index(drop=True),
        df_gal.iloc[idx_gal].reset_index(drop=True).add_suffix('_GAL')
    ], axis=1)

    # Açısal uzaklığı da dosyaya ekleyelim
    matched['separation_arcmin'] = d2d.to(u.arcmin).value

    # --- D) Redshift Filtresi (Kritik Adım) ---
    # SN ve Galaksi redshift farkı 0.005'ten küçük olmalı
    before_filter = len(matched)
    matched = matched[abs(matched['zCMB'] - matched['redshift_GAL']) <= MAX_REDSHIFT_DIFF]
    after_filter = len(matched)

    print(f"Eşleşme Sonucu: {before_filter} -> Filtre Sonrası: {after_filter} veri kaldı.")

    # --- E) Feature Engineering (Renkleri Hesapla ve Kaydet) ---
    matched['u_g'] = matched['u_GAL'] - matched['g_GAL']
    matched['g_r'] = matched['g_GAL'] - matched['r_GAL']
    matched['r_i'] = matched['r_GAL'] - matched['i_GAL']
    matched['i_z'] = matched['r_GAL'] - matched['z_GAL']

    # --- F) CSV Olarak Kaydet ---
    output_filename = 'final_winning_dataset_3arcmin.csv'
    matched.to_csv(output_filename, index=False)

    # Hata veren satır burasıydı, düzeltildi:
    print(f"\n✅ BAŞARILI! Dosya '{output_filename}' adıyla kaydedildi.")
    return output_filename

# ==========================================
# 4. ÇALIŞTIRMA VE İNDİRME
# ==========================================
created_file = create_and_save_dataset()

# Eğer Colab kullanıyorsanız dosyayı bilgisayarınıza indirmek için:
try:
    files.download(created_file)
    print("İndirme işlemi başlatıldı...")
except:
    print("Dosya Google Drive veya yerel dizine kaydedildi. Sol panelden indirebilirsiniz.")

import pandas as pd
from catboost import CatBoostRegressor

# 1. Hazır Veri Setini Yükle
df_train = pd.read_csv('final_winning_dataset_3arcmin.csv')

# 2. Özellikleri Belirle
features = ['u_GAL', 'g_GAL', 'r_GAL', 'i_GAL', 'z_GAL', 'u_g', 'g_r', 'r_i', 'i_z']
target = 'MU_SH0ES'

X = df_train[features]
y = df_train[target]

# 3. Modeli Eğit (En iyi parametrelerle)
print("Model eğitiliyor ve kaydediliyor...")
model = CatBoostRegressor(iterations=1000, depth=6, learning_rate=0.05, verbose=0, random_state=42)
model.fit(X, y)

# 4. Modeli Dosyaya Kaydet
model_filename = 'supernova_predictor_model.cbm'
model.save_model(model_filename)
print(f"✅ Model '{model_filename}' adıyla başarıyla kaydedildi.")

!pip install catboost astropy pandas numpy matplotlib scikit-learn -q
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob
import os
from catboost import CatBoostRegressor

# ==========================================
# 1. AYARLAR
# ==========================================
# Galaksi dosyalarınızın olduğu klasör (Örn: 'galaxy_data/')
RAW_DATA_FOLDER = '/content/drive/MyDrive/tez/galaxy_data/'  # Dosyalar bu kodla aynı yerdeyse './' kalsın
# Eğittiğimiz model dosyasının yolu
MODEL_PATH = 'supernova_predictor_model.cbm'

# ==========================================
# 2. VERİ YÜKLEME VE İŞLEME
# ==========================================
print("Dosyalar taranıyor...")
all_files = glob.glob(os.path.join(RAW_DATA_FOLDER, "*.csv"))

if not all_files:
    print("HATA: Klasörde .csv dosyası bulunamadı!")
else:
    print(f"Toplam {len(all_files)} adet dosya bulundu. İşleniyor...")

df_list = []

for f in all_files:
    try:
        # Dosyayı oku (Virgül ile ayrılmış standart format varsayımıyla)
        # Eğer dosya noktalı virgül ise sep=';' yapılmalı, ancak yüklediğiniz örnek virgüllü.
        temp_df = pd.read_csv(f)

        # Sütun isimlerini temizle (boşluk varsa sil)
        temp_df.columns = temp_df.columns.str.strip()

        # Gerekli sütun kontrolü
        required_cols = ['u', 'g', 'r', 'i', 'z', 'redshift']
        if not set(required_cols).issubset(temp_df.columns):
            # Bazı dosyalarda 'redshift' yerine 'z1' veya farklı isim olabilir
            # Eğer 'redshift' yoksa ve 'z1' varsa ismini değiştirelim
            if 'z1' in temp_df.columns:
                temp_df = temp_df.rename(columns={'redshift'})
            else:
                print(f"UYARI: {f} dosyasında eksik sütunlar var, atlanıyor.")
                continue

        # Sayısal dönüşüm (Garanti olması için)
        for c in ['u', 'g', 'r', 'i', 'z', 'redshift']:
            temp_df[c] = pd.to_numeric(temp_df[c], errors='coerce')

        # Boş verileri temizle
        temp_df = temp_df.dropna(subset=['u', 'g', 'r', 'i', 'z'])

        df_list.append(temp_df)

    except Exception as e:
        print(f"HATA: {f} dosyası okunurken sorun oluştu: {e}")

if df_list:
    full_df = pd.concat(df_list, ignore_index=True)
    print(f"\nToplam {len(full_df)} galaksi verisi birleştirildi.")

    # ==========================================
    # 3. ÖZELLİK MÜHENDİSLİĞİ (FEATURE ENGINEERING)
    # ==========================================
    # Sütun isimlerini modelin beklediği formata (u_GAL vb.) çevir
    # Not: Orijinal dosyadaki 'u', 'g' sütunlarını koruyarak kopyasını alıyoruz
    full_df['u_GAL'] = full_df['u']
    full_df['g_GAL'] = full_df['g']
    full_df['r_GAL'] = full_df['r']
    full_df['i_GAL'] = full_df['i']
    full_df['z_GAL'] = full_df['z']

    # Renk Farklarını Hesapla (Eğitimdeki mantıkla BİREBİR AYNI)
    full_df['u_g'] = full_df['u_GAL'] - full_df['g_GAL']
    full_df['g_r'] = full_df['g_GAL'] - full_df['r_GAL']
    full_df['r_i'] = full_df['r_GAL'] - full_df['i_GAL']
    # DİKKAT: Eğitim kodunda i_z özelliği (r - z) olarak hesaplanmıştı:
    full_df['i_z'] = full_df['r_GAL'] - full_df['z_GAL']

    # Modelin beklediği özellik listesi
    features = ['u_GAL', 'g_GAL', 'r_GAL', 'i_GAL', 'z_GAL', 'u_g', 'g_r', 'r_i', 'i_z']

    # ==========================================
    # 4. TAHMİN YAPMA
    # ==========================================
    if os.path.exists(MODEL_PATH):
        print("\nModel yükleniyor ve tahmin yapılıyor...")
        model = CatBoostRegressor()
        model.load_model(MODEL_PATH)

        # Tahminleri yap
        preds = model.predict(full_df[features])
        full_df['TAHMIN_MU'] = preds

        # ==========================================
        # 5. SONUÇLARI GÖRSELLEŞTİRME (KONTROL)
        # ==========================================
        plt.figure(figsize=(10, 6))

        # Renk kodlaması olarak g-r (galaksi rengi) kullanalım
        scatter = plt.scatter(full_df['redshift'], full_df['TAHMIN_MU'],
                             c=full_df['g_r'], cmap='viridis', s=2, alpha=0.5)

        plt.colorbar(scatter, label='Renk (g-r)')
        plt.xlabel('Redshift (z)')
        plt.ylabel('Model Tahmini: Mesafe Modülü (MU_SH0ES)')
        plt.title('Model Performansı: Ham Veri Üzerinde Hubble Diyagramı')
        plt.grid(True, alpha=0.3)
        plt.show()

        print("\nGrafik Yorumu:")
        print("- Eğer noktalar sol alttan sağ üste doğru ince bir şerit oluşturuyorsa modeliniz bu ham verilerde de tutarlı çalışıyor demektir.")
        print("- Dağınıklık varsa, ham verideki bazı galaksiler modelin eğitim setine benzemiyor olabilir (outlier).")

        # Sonuçları Kaydet
        output_name = 'tum_galaksi_tahminleri.csv'
        save_cols = ['objid', 'ra', 'dec', 'redshift', 'TAHMIN_MU']
        # Eğer objid yoksa indeksi kullan
        if 'objid' not in full_df.columns:
            full_df['objid'] = full_df.index

        full_df[save_cols].to_csv(output_name, index=False)
        print(f"\nTahminler '{output_name}' dosyasına kaydedildi.")

    else:
        print(f"UYARI: Model dosyası ({MODEL_PATH}) bulunamadı. Lütfen dosya yolunu kontrol edin.")
else:
    print("İşlenecek veri bulunamadı.")

!pip install catboost astropy pandas numpy matplotlib scikit-learn -q
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob
import os
from catboost import CatBoostRegressor

# ==========================================
# 1. AYARLAR
# ==========================================
RAW_DATA_FOLDER = '/content/drive/MyDrive/tez/galaxy_data/'          # CSV dosyalarının olduğu klasör
MODEL_PATH = 'supernova_predictor_model.cbm'
Z_BIN_SIZE = 0.005              # Dilim genişliği (Redshift)

# ==========================================
# 2. VERİ YÜKLEME VE İŞLEME
# ==========================================
print("Dosyalar okunuyor...")
all_files = glob.glob(os.path.join(RAW_DATA_FOLDER, "*.csv"))

df_list = []
for f in all_files:
    try:
        # Virgül ile ayrılmış varsayımı (Yüklediğiniz son dosya virgüllüydü)
        temp_df = pd.read_csv(f)
        temp_df.columns = temp_df.columns.str.strip()

        # Gerekli sütun kontrolü ve isim düzeltme
        if 'z1' in temp_df.columns and 'redshift' not in temp_df.columns:
            temp_df = temp_df.rename(columns={'z1': 'redshift'})

        required = ['u', 'g', 'r', 'i', 'z', 'redshift']
        if set(required).issubset(temp_df.columns):
             # Sayısal dönüşüm ve temizlik
            for c in required:
                temp_df[c] = pd.to_numeric(temp_df[c], errors='coerce')
            temp_df = temp_df.dropna(subset=required)
            df_list.append(temp_df)
    except Exception as e:
        print(f"Hata ({f}): {e}")

if not df_list:
    print("İşlenecek veri bulunamadı.")
    exit()

full_df = pd.concat(df_list, ignore_index=True)
print(f"Toplam {len(full_df)} galaksi işleniyor.")

# ==========================================
# 3. ÖZELLİK MÜHENDİSLİĞİ & TAHMİN
# ==========================================
# Sütunları model formatına çevir
full_df = full_df.rename(columns={'u': 'u_GAL', 'g': 'g_GAL', 'r': 'r_GAL', 'i': 'i_GAL', 'z': 'z_GAL'})

# Renkleri Hesapla
full_df['u_g'] = full_df['u_GAL'] - full_df['g_GAL']
full_df['g_r'] = full_df['g_GAL'] - full_df['r_GAL']
full_df['r_i'] = full_df['r_GAL'] - full_df['i_GAL']
full_df['i_z'] = full_df['r_GAL'] - full_df['z_GAL'] # Model eğitimindeki mantık (r-z)

# Model Yükle ve Tahmin Et
features = ['u_GAL', 'g_GAL', 'r_GAL', 'i_GAL', 'z_GAL', 'u_g', 'g_r', 'r_i', 'i_z']

if os.path.exists(MODEL_PATH):
    print("Model yükleniyor...")
    model = CatBoostRegressor()
    model.load_model(MODEL_PATH)

    # Her galaksi için bireysel tahmin
    full_df['TAHMIN_MU'] = model.predict(full_df[features])
else:
    print("Model dosyası bulunamadı!")
    exit()

# ==========================================
# 4. BINNING (DİLİMLEME) VE ORTALAMA ALMA
# ==========================================
print(f"\nVeriler {Z_BIN_SIZE} aralıklı redshift dilimlerine ayrılıyor...")

# Min ve Max redshift aralığını belirle
z_min = full_df['redshift'].min()
z_max = full_df['redshift'].max()

# Dilim sınırlarını oluştur (Örn: 0.0, 0.005, 0.010, ...)
bins = np.arange(0, z_max + Z_BIN_SIZE, Z_BIN_SIZE)

# Her galaksinin hangi dilime düştüğünü bul
full_df['z_bin'] = pd.cut(full_df['redshift'], bins)

# GRUPLA ve ORTALAMA AL
# Her dilim için: Redshift'in ortalamasını ve Tahmin'in ortalamasını alıyoruz.
binned_results = full_df.groupby('z_bin').agg({
    'redshift': 'mean',          # Dilimin merkezini bulmak için
    'TAHMIN_MU': ['mean', 'std', 'count'] # Tahmin ortalaması, sapması ve galaksi sayısı
}).reset_index()

# Sütun isimlerini düzelt
binned_results.columns = ['z_range', 'mean_z', 'mean_pred_mu', 'std_pred_mu', 'count']

# Boş dilimleri temizle (Hiç galaksi olmayan aralıklar)
binned_results = binned_results.dropna(subset=['mean_z'])

# Sonuçları Kaydet
binned_results.to_csv('z_dilimli_tahmin_sonuclari.csv', index=False)
print("Sonuçlar 'z_dilimli_tahmin_sonuclari.csv' dosyasına kaydedildi.")

# ==========================================
# 5. GÖRSELLEŞTİRME
# ==========================================
plt.figure(figsize=(12, 7))

# Ortalama Tahmin Çizgisi
plt.plot(binned_results['mean_z'], binned_results['mean_pred_mu'],
         color='red', linewidth=2, label='Ortalama Tahmin (Binned Mean)')

# Standart Sapma Alanı (Güven Aralığı gibi düşünülebilir)
plt.fill_between(binned_results['mean_z'],
                 binned_results['mean_pred_mu'] - binned_results['std_pred_mu'],
                 binned_results['mean_pred_mu'] + binned_results['std_pred_mu'],
                 color='red', alpha=0.2, label='Standart Sapma (Dağılım)')

# Arka plana bireysel galaksileri de sönük olarak atalım (görmek için)
# Çok veri varsa yavaşlatabilir, örneklem alıyoruz
sample = full_df.sample(n=min(len(full_df), 10000))
plt.scatter(sample['redshift'], sample['TAHMIN_MU'],
            c='gray', s=1, alpha=0.1, label='Bireysel Galaksiler (Örneklem)')

plt.xlabel('Redshift (z)')
plt.ylabel('Mesafe Modülü (MU_SH0ES)')
plt.title(f'Redshift Dilimlerine Göre Ortalama Model Tahmini (Bin Size: {Z_BIN_SIZE})')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# İlk 10 dilimi ekrana bas
print("\n--- Dilimlenmiş Sonuçlar (İlk 10) ---")
print(binned_results[['mean_z', 'mean_pred_mu', 'count']].head(10))